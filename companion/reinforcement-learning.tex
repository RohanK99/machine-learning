\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage[makeindex]{imakeidx}
\usepackage{titlesec}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{xcolor, soul}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{parskip}
\usepackage[ruled,vlined]{algorithm2e}

\graphicspath{ {./images/} }

\definecolor{linkColour}{RGB}{140, 25, 57}
\definecolor{urlColour}{RGB}{137, 62, 27}

\setcounter{section}{-1}
\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\hypersetup{colorlinks, citecolor=purple, filecolor=purple, linkcolor=linkColour, urlcolor=urlColour}

\makeindex

\newenvironment{fact*}[2][]
    {
    \begin{adjustwidth}{1em}{0em}
    \noindent
    \textbf{#2} \hfill #1
    
    \vspace{0.1in}
    \noindent
    \ignorespaces
    } {
    \end{adjustwidth}
    }

\newenvironment{fact}[2][]
    {
    \index{#2}
    \hypertarget{#2}{\vspace{0.2in}}
    \begin{adjustwidth}{1em}{0em}
    \noindent
    \textbf{#2} \hfill #1
    
    \vspace{0.1in}
    \noindent
    \ignorespaces
    } {
    \end{adjustwidth}
    }

\title{Companion to Reinforcement Learning}
\author{Rohan Kumar}
\date{}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Notation}
    \subsection{Data}
    \begin{flalign*}
        \boldsymbol{x} &= \begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_M \end{pmatrix} \text{: data point corresponding to a
        column vector of $M$ features} & \\
        \overline{\boldsymbol{x}} &= \begin{pmatrix} 1 \\ x_1 \\ x_2 \\ ... \\ x_M \end{pmatrix} \text{: concatenation
        of 1 with the vector} \boldsymbol{x} & \\
        \boldsymbol{X} &= \begin{pmatrix} x_{1,1} & ... & x_{1,N} \\ ... & ... & ... \\ x_{M,1} & ... & x_{M,N}
        \end{pmatrix} \text{: dataset consisting of $N$ data points and $M$ features} & \\
        \overline{\boldsymbol{X}} &= \begin{pmatrix} 1 & ... & 1 \\ x_{1,1} & ... & x_{1,N} \\ ... & ... & ... \\
        x_{M,1} & ... & x_{M,N} \end{pmatrix} \text{: concatenation of a vector of 1's with the matrix } \boldsymbol{X}
        & \\
        y &= \text{: output target (regression) or label (classification)} & \\
        \boldsymbol{y} &= \begin{pmatrix} y_1 \\ y_2 \\ ... \\ y_N \end{pmatrix} \text{: vector of outputs for a dataset
        of $N$ points} & \\
        \boldsymbol{x}_* &= \text{: test input / unknown input } & \\
        \boldsymbol{y}_* &= \text{: predicted output} & \\
        N &= \text{: Number of data points in the dataset} & \\
        M &= \text{: Number of a features in a data point} & \\
        \boldsymbol{w} &= \begin{pmatrix} w_1 \\
            w_2 \\
            ... \\
            w_M \\
        \end{pmatrix} & \\
        \boldsymbol{w}^T &= (w_1, w_2, ..., w_M) \text{ or } (w_0, w_1, w_2, ..., w_M) \text{ $w_0$ multiplies the first
        entry of $\overline{\boldsymbol{x}}$ (bias)} & \\
        * &= \text{optimal policy and or function}
    \end{flalign*}
    Note: bold symbols represents a vector

\section{Introduction}
    Reinforcement Learning is an area of machine learning inspired by behavioural psychology, concerned with how
    software \textbf{agents} ought to take \textbf{actions} in an \textbf{environment} so as to maximize some notion of
    cumulative \textbf{reward}. 


\section{Markov Processes}
    Markov processes are important to model environment dynamics and to model this we will use stochastic processes and
    the two important assumptions we will consider are the Markovian Assumption and the Stationary Assumption. 

    \subsection{Unrolling the Problem}
        If we consider the problem of reinforcement learning where we take an action based on the state and receive a
        reward then we can unroll the loop into a sequence of states, actions and rewards:

        $$ s_0, a_0, r_0, s_1, a_1, r_1, s_2, a_2, r_2 $$

        This sequence forms a stochastic process (due to some uncertainty in the dynamics of the process). This is
        because we don't know the underlying state of $s_0$ and therefore we don't know the correct action $a_0$ that
        will lead to the next optimal state. 

    \subsection{Stochastic Processes}
        Processes are rarely arbitrary and very often exhibit structure. The laws of the process do not change and the
        short history is sufficient to predict the future. 

        \subsubsection{Definition}
            We will define a stochastic process with respect to states. We let $S$ be our set of states and we define
            the stochastic dynamics to be $P(s_t | s_{t-1}, ..., s_0)$ which in general expresses some conditional
            distribution over the current state given the past states.

        \subsubsection{Problem}
            The problem that arises with this is that if we have a process that is very long then this conditional
            distribution that produces the state at time step $t$ could depend on a number of states before. Expressing
            a large number of previous states results in an intractable problem. If the process was infinitely long and
            every state depended on everything that happened before then the problem cannot be expressed. 

            The solution to this is to assume that the process is stationary: the dynamics do not change over time. The
            other assumption we need to make is the Markov assumption which is that the current state depends only on a
            finite history of past states. 
        
    \subsection{K-order Markov Process}
        Here we use the Markov Assumption and assume that the last $k$ states is sufficient. In a first-order markov
        process we assume that only the last state is relevant and sufficient.

        By default a Markov Process refers to a first order process
        $$ P(s_t | s_{t-1}, s_{t-2}, ..., s_0) = P(s_t | s_{t-1}) \forall t $$
        and expresses the stationary assumption that
        $$ P(s_t | s_{t-1}) = P(s_{t'} | s_{t' - 1}) \forall t' $$

        The stationary assumption means that the conditional distribution is going to be the same regardless of which
        time step we consider. 

        The advantage of this is we can now specify an entire process with a single concise conditional distribution
        $$ P(s' | s) $$

    \subsection{Non-Markovian and Non-Stationary Processes}
        If the process is not Markovian and/or not stationary then we can add new state components until the dynamics
        are Markovian and stationary. For example considering the dynamics in robotics we have state $<x, y, z,
        \theta>$, this is not stationary when velocity varies. The solution is to add velocity to the state description
        as $<x, y, z, \theta, \dot{x}, \dot{y}, \dot{z}, \dot{\theta}>$

        The problem with this solution is that adding components to the state description to force a process to be
        Markovian and stationary may significantly increase computational complexity. The solution to this would be to
        try to find the smallest state description that is self-sufficient (Markovian and stationary).

    \subsection{Inference in Markov Processes}
        Assuming we have a Markovian Process that is stationary, we want to do inference to predict what will be the
        value of some future state and the reason this is important is because in a Markov decision process and more
        generally in reinforcement learning the goal is to select actions that will influence future states and get us
        into states that will have high rewards. Therefore if we can predict what the future state is going to be then
        we can select some good actions. 

        Predicting a state $k$ time steps into the future:
        $$ P(s_{t+k} | s_t) $$

        To compute this we perform the following which takes advantage of the chain rule in probability theory to expand
        this into a product to sum out all the intermediate states:
        $$ P(s_{t+k} | s_{t}) = \sum_{s_{t+1} ... s_{t+k-1}} \prod^{k}_{i=1} P(s_{t+i} | s_{t+i-1}) $$

        If we have discrete states (finitely many states) then we can represent the conditional distribution as a matrix
        and here we use the letter $T$ to indicate a transition matrix. So we let $T$ be a $|S| \times |S|$ matrix
        representing $P(s_{t+1} | s_{t})$. Then $P(s_{t+k} | s_t) = T^k$. What we're doing here is that we are
        essentially taking the product of that matrix $k$ times. Complexity: $O(k |S|^3)$.

\section{Markov Decision Processes}
    For a Markov Decision Process we will augment the Markov process with Actions and Rewards. Our goal is to select
    actions that will influence the future states in a good way. The choice of actions depends on the current state. The
    reward depends on the current state and action, this quantifies how good it is to be in a certain state and execute
    a certain action.

    The current assumptions we need to maintain are it being a stochastic process, sequential process, fully observable
    states, complete model and the process to be discrete.

    \subsection{Definition}
        We define the set of states to be $S$ and the set of actions to be $A$. We consider a transition model $P(s_t,
        s_{t-1}, a_{t-1})$ that corresponds to the stochastic process model with the addition of an action. We have the
        reward model $R(s_t, a_t)$ and its discount factor $\gamma$. Finally we have a horizon $h$, which is the time
        horizon/number of time steps. The goal is to find an optimal policy, a mapping from states to actions. 

    \subsection{Rewards}
        The reward is a real number ($r_t \in \mathbb{R}$) that  we are going to try and maximize. It is essentially a numerical signal that
        indicates how good the state and action is at every time step. We can express the reward function as $R(s_t,
        a_t) = r_t$. 

        Another common assumption we make here is that the reward function is stationary where $R(s_t, a_t)$ is the same
        at every time step ($\forall t$). This does not mean the reward is the same at every time step just the function
        is the same. The exception to this is the terminal reward function is often different (e.g. in a game, 0 reward
        at each turn but +1/-1 at the end for winning/losing).

        The goal is to maximize the sum of the rewards $\sum_t R(s_t, a_t)$.

        \subsubsection{Discounted/Average Rewards}
            If process is infinite and my rewards are always positive then $\sum_t R(s_t, a_t)$ approaches infinity.
            This becomes an issue. 

            The first solution here is to consider discounted rewards. We introduce a discount factor $0 \leq \gamma
            \leq 1$. The idea here is that we want to discount the reward function by $\gamma^t$ where $t$ is the time
            step. We can represent the finite utility as $\sum_t \gamma^t R(s_t, a_t)$. The rewards further in the
            future will be discounted more. We can think of $\gamma$ as an inflation rate of $\frac{1}{\gamma - 1}$. The
            intuition here is that we prefer utility sooner than later.

            The second solution is to average the rewards. in some problems we don't want to discount earning a reward
            thousands of time steps into the future rather it should have the same reward as earning the utility now. In
            this case we can average the reward. This is computationally more complicated and will not currently be
            considered in this literature. 

\section{Dynamic Programming Algorithms for Solving MDPs}
    \subsection{Policy and Policy Optimization}
        A policy is a formal definition for how to select actions. We will denote a policy using $\pi$. We are mapping
        states to actions so we can represent this as $\pi(s_t) = a_t$. The goal in reinforcement learning and markov
        processes is to come up with this policy $\pi$. We are going to make the assumption that we have fully
        observable states, so therefore we can condition the choice of action basely on the current state $s_t$.

        We now define the expected utility as:
        
        $$ V^{\pi}(s_0) = \sum_{t=0}^h \gamma^t \sum_{s_t} P(s_t | s_0, \pi) R(s_t, \pi(s_t)) $$

        Here we have a discounted cumulative reward where we have a sum with respect to all time steps from 0 to $h$
        (horizon). Then our reward at each time step is $R(s_t, \pi (s_t))$ which depends on the current state. That
        state has some uncertainty in it and it really depends on the transition dynamics that are stochastic and
        therefore we have some expectation $P(s_t | s_0, \pi)$.

        We represent the optimal policy as $\pi^*$ which is the policy with the highest expected utility where the
        following holds:
        $$ V^{\pi^*} (s_0) \geq V^{\pi} (s_0) \forall \pi$$

        There are several classes of algorithms for policy optimizations:
        \begin{itemize}
            \item Value iteration
            \item Policy iteration
            \item Linear programming
            \item Search techniques
        \end{itemize}

        Computation may be done offline: before the process starts or online: as the process evolves.
    
    \subsection{Value Iteration} \label{sec:ValueIteration}
        The way this algorithm works is by optimizing / selecting actions in reverse order. To find what the best
        action at every step we can use dynamic programming that will work backwards and optimize the last decision
        then the second last decision then so on. 

        We can demonstrate this mathematically as follows:
        
        We start at the last time step $h$.
        $$ V(s_h) = max_{a_h} R(s_h, a_h) $$

        To optimize the last decision is easy we look at all possible actions and pick the one that yields the
        highest reward. 

        Value with one time step left:
        $$ V\left(s_{h-1}\right)=\max _{a_{h-1}} R\left(s_{h-1}, a_{h-1}\right)+\gamma \sum_{s_{h}}
        P(s_{h} \mid s_{h-1}, a_{h-1}) V\left(s_{h}\right) $$

        Here we are taking into account the reward for the second last time step as well as the last time step and
        sum the rewards. We have the second last reward $\max _{a_{h-1}} R\left(s_{h-1}, a_{h-1}\right)$ plus the
        discount factor $\gamma$ multiplied into the expectation $P(s_{h} \mid s_{h-1}, a_{h-1})$ with respect to
        what the last state will be times the value that I can earn in the last state $V(s_h)$. Now we simply select
        $a_{h-1}$ to maximize this where $V(s_h)$ is already maximized.

        Similarly value with two time steps left:
        $$ V\left(s_{h-2}\right)=\max _{a_{h-2}} R\left(s_{h-2}, a_{h-2}\right)+\gamma \sum_{s_{h-1}} P(s_{h-1} \mid
        s_{h-2}, a_{h-2}) V\left(s_{h-1}\right) $$

        If we do this for the entire planning horizon we can generalize to Bellman's equation:
        $$ V(s_t) = max_{a_t} R(s_t, a_t) + \gamma \sum_{s_t + 1} P(s_{t+1} \mid s_t, a_t) V(s_t + 1) $$

        Now we can represent the equation to get the optimal action to construct the optimal policy is:
        $$ a_t^* = argmax_{a_t} R(s_t, a_t) + \gamma \sum_{s_t + 1} P(s_{t+1} \mid s_t, a_t) V(s_t + 1) $$

        \textbf{Finite Horizon}
            When $h$ is finite we have a non-stationary optimal policy where the best action is different at every
            time step. The intuition here is that the best action varies with the amount of time left.

        \textbf{Infinite Horizon}
            When $h$ is infinite, stationary is the optimal policy. We pick the same best action at each time step.
            The intuition here is that the same amount of time (infinite) at each time step, therefore we pick the
            same best action. The main problem with this is that value iteration will do an infinite number of
            iterations. 

            The solution is that we can perform value iteration using a finite $n$ since we are using a discount
            factor $\gamma$. After $n$ time steps the rewards are scaled down by $\gamma^n$ and for a large enough
            $n$, the rewards become insignificant since $\gamma^n \rightarrow 0$. For this we need to pick a large
            enough $n$, run value iteration for $n$ steps and execute policy found at the $n^{th}$ iteration.

        \subsubsection{Algorithm}
            \begin{algorithm}[H]
                \SetAlgoLined
                $V_0^* \gets max_a R(s,a) \forall s$\;
                \For{$t\gets1$ \KwTo $h$} {
                    $V_t^*(s) \gets max_a R(s,a) + \gamma \sum_{s'}P(s' | s,a)V_{t-1}^* \forall s$
                }
                \Return $V^*$
                \caption{Value Iteration MDP}
            \end{algorithm}

            We get the optimal policy $\pi^*$:
            \begin{itemize}
                \item $t=0$; $\pi_0^*(s) \gets argmax_a R(s,a) \forall s$
                \item $t>0$; $\pi_t^*(s) \gets argmax_a R(s,a) + \gamma \sum_{s'}P(s' \mid s, a) V_{t-1}^* (s') \forall s$
            \end{itemize}
            
            In this case $t$ indicates the number of time steps to go till the end of process and $\pi^*$ is non
            stationary. 
            
            We can express this in matrix form as follows. Let:
            \begin{itemize}
                \item $R^a$: $|S| \times 1$ column vector of rewards for $a$.
                \item $V_t^*$: $|S| \times 1$ column vector of state values.
                \item $T^a$: $|S| \times |S|$ matrix of transition probabilities for $a$.
            \end{itemize}

            \begin{algorithm}[H]
                \SetAlgoLined
                $V_0^* \gets max_a R^a$\;
                \For{$t\gets1$ \KwTo $h$} {
                    $V_t^* \gets max_a R^a + \gamma T^a V_{t-1}^*$
                }
                \Return $V^*$
                \caption{Value Iteration MDP}
            \end{algorithm}

            Even when the horizon is infinite we can perform finitely many iterations using the principle of
            contraction and convergence. We can modify the algorithm to stop when $||V_n - V_{n-1}|| \leq \epsilon$.
            
            \begin{algorithm}[H] \label{InfiniteValIterAlgo}
                \SetAlgoLined
                $V_0^* \gets max_a R^a$\;
                $n \gets 0$\;
                \SetKwRepeat{Do}{do}{while}
                \Do {
                    $||V_n - V_{n-1}||_{\infty} \leq \epsilon ||$
                } {$n \gets n + 1$\;
                $V_n^* \gets max_a R^a + \gamma T^a V_{n-1}^*$\;}
                \Return $V^*$
                \caption{Value Iteration MDP}
            \end{algorithm}

            The complexity of the value iteration algorithm is that each iteration is $O(|S|^2|A|)$.

        \subsection{Policy Evaluation}
            Policy evaluation computes the value functions for a policy $\pi$. If we consider policy evaluation for an
            infinite horizon where we let $h \rightarrow \infty$ then $V_h^{\pi} \rightarrow V_{\infty}^{\pi}$ and
            $V_{h-1}^{\pi} \rightarrow V_{\infty}^{\pi}$
            
            Then the equation for policy evaluation becomes:
            $$V_{\infty}^{\pi}(s)=R\left(s, \pi_{\infty}(s)\right)+\gamma \sum_{s^{\prime}} \operatorname{P}\left(s^{\prime} \mid s, \pi_{\infty}(s)\right) V_{\infty}^{\pi}\left(s^{\prime}\right) \forall s$$

            Bellman's equation:
            $$V_{\infty}^{*}(s)=\max _{a} R(s, a)+\gamma \sum_{s^{\prime}} \operatorname{P}\left(s^{\prime} \mid s, a\right) V_{\infty}^{*}\left(s^{\prime}\right)$$

            Representing the policy evaluation in matrix form where:
            \begin{itemize}
                \item $R$: $|S| \times 1$ column vector of state rewards for $\pi$.
                \item $V$: $|S| \times 1$ column vector of state rewards for $\pi$.
                \item $T$: $|S| \times |S|$ column vector of state rewards for $\pi$.
            \end{itemize}

            then we get:
            $$ V = R + \gamma TV $$

            If we consider Bellman's equation for a set of non-linear equations then we get:
            $$ V^* = max_a R^a + \gamma T^a V^* $$

            To find the value of the policy we solve the system. We can do it using the following
            methods:

            \begin{itemize}
                \item Gaussian Elimination: $(I - \gamma T)V = R$
                \item Compute Inverse: $V = (I - \gamma T)^{-1}R$
                \item Iterative Methods \begin{itemize}
                    \item Value Iteration (Richardson Iteration)
                    \item Repeat $V \gets R + \gamma TV$
                \end{itemize}
            \end{itemize}
            
            \subsubsection{Induced Policy}
                Using the \hyperref[InfiniteValIterAlgo]{infinite value iteration algorithm} for an infinite horizon we
                can derive the station policy $\pi_n(s)$ based on the computed optimal $V_n$ using:

                $$ \pi_n(s) = argmax_a R(S,a) + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s, a) V_n(s^{\prime}) $$

                We can further prove that $||V^{\pi_n} - V^*||_{\infty} \leq \frac{2 \epsilon}{1-\gamma}$. That is how
                far $V^{\pi_n}$ (value of the policy) is from $V^*$. 

    \subsection{Policy Iteration}
        Instead of doing \hyperref[sec:ValueIteration]{value iteration} where we optimize the value function and extract
        the induced policy, we can directly optimize the policy using \textbf{policy iteration}. An advantage of policy
        iteration is that the number of iterations required tends to be small in practice.
        
        \subsubsection{Algorithm}
            The algorithm for policy iteration alternates between 2 steps
            \begin{enumerate}
                \item Policy Evaluation
                $$ V^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s, \pi(s)) V^{\pi}(s^{\prime})
                \forall s $$
                \item Policy Improvement
                $$ \pi(s) \gets argmax_a R(s,a) + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s,a) V^{\pi}(s^{\prime})
                \forall s $$
            \end{enumerate}
            
            \begin{algorithm}[H] \label{PolicyIterationAlgo}
                \SetAlgoLined
                Initialize $\pi_0$ to any policy. \;
                $n \gets 0$ \;
                \SetKwRepeat{Do}{do}{while}
                \Do {
                    $\pi_{n+1} = \pi_n$
                } {
                    Eval: $V_n = R^{\pi_n} + \gamma T^{\pi_n} V_n$ \;
                    Improve: $\pi_{n+1} \gets argmax_a R^a + \gamma T^a V_n$ \;
                    $n \gets n + 1$
                }
                \Return $\pi_n$
                \caption{Policy Iteration MDP}
            \end{algorithm}

            The complexity of each iteration in policy iteration is $O(|S|^3 + |S|^2|A|)$, however, it has
            linear-quadratic convergence which requires fewer iterations than value iteration.

        \subsubsection{Modified Policy Iteration Algorithm}
            We want to find a modified algorithm that has a complexity similar to that of value iteration and a
            convergence similar to that of the original policy iteration algorithm. This modified algorithm follows the
            same principle as policy iteration, however, in order to reduce the cost per iteration it will do a partial
            policy evaluation. 

            The algorithm for policy iteration alternates between 2 steps
            \begin{enumerate}
                \item Partial Policy Evaluation. Repeat $k$ times:
                $$ V^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s, \pi(s)) V^{\pi}(s^{\prime})
                \forall s $$
                \item Policy Improvement
                $$ \pi(s) \gets argmax_a R(s,a) + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s,a) V^{\pi}(s^{\prime})
                \forall s $$
            \end{enumerate}
            
            \begin{algorithm}[H] \label{ModifiedPolicyIterationAlgo}
                \SetAlgoLined
                Initialize $\pi_0$ and $V_0$ to anything. \;
                $n \gets 0$ \;
                \SetKwRepeat{Do}{do}{while}
                \Do {
                    $||V_n - V_{n-1}||_{\infty} \leq \epsilon$
                } {
                    Eval: Repeat $k$ times \;
                    \Indp $V_n = R^{\pi_n} + \gamma T^{\pi_n} V_n$ \;
                    \Indm Improve: $\pi_{n+1} \gets argmax_a R^a + \gamma T^a V_n$ \;
                    \Indp $V_{n+1} \gets max_a R^a + \gamma T^a V_n$ \;
                    \Indm $n \gets n + 1$ \;
                }
                \Return $\pi_n$
                \caption{Modified Policy Iteration MDP}
            \end{algorithm}

            The complexity of each iteration is now $O(k|S|^2 + |S|^2|A|)$ which is less than the cost for the original
            policy iteration and number of iterations for convergence will be linear-quadratic convergence (more
            iterations than the original policy iteration but less than that of value iteration).

\section{Reinforcement Learning}
    \subsection{Introduction}
        We can represent the problem of reinforcement learning as a MDP. We are going to relax the assumption of the
        reward model from being stationary to being stochastic and modify the goal. If we consider the Reinforcement
        learning problem the policy decides what action to commit and the transition and reward model determine the
        state and reward. The main difference is that we don't necessarily know what the state model is, or what rewards
        the environment uses. So formally we can define reinforcement learning as follows:

        \begin{itemize}
            \item States: $s \in S$
            \item Actions: $a \in A$
            \item Rewards: $r \in \mathbb{R}$
            \item Transition Model: $P(s_t \mid s_{t-1}, a_{t-1})$ (unknown)
            \item Reward Model: $P(r_t \mid s_t, a_t)$
            \item Discount Factor: $0 \leq \gamma \leq 1$
            \item Horizon: $h$
        \end{itemize}

        Goal: Find optimal policy $\pi^*$ such that
        $$ \pi^* = argmax_{\pi} \sum_{t=0}^h \gamma^t E_{\pi}[r_t] $$

        We will now go in depth about algorithms to achieve this policy with the constraints. The idea is to learn
        an optimal policy while interacting with the environment. The intuition behind this is that we are going to
        observe the state's actions and rewards at every step and then this gives us samples effectively from the
        unknown models. When we have a large enough sample there is presumably enough information to recover the model
        or come up with policy that would be optimal with respect to these unknown models.

    \subsection{RL Agent Architectures}
        Most agents are a combination of 3 important components.
        
        \textbf{Model}: $P(s^{\prime} \mid s, a)$, $P(r \mid s, a)$ -
        Some agents are going to learn a model explicitly so they are considered model based agents. This involves
        transition dynamics and reward distribution.

        \textbf{Policy}: $\pi(s)$ -
        Some agents are also explicitly going to model a policy, so that's the choices that the agents make in different
        states.

        \textbf{Value Function}: $V(s)$ -
        Some agents will explicitly write the value function which is simply the expected total sum of the rewards. 

        \subsubsection{Categorizing RL Agents}
            \begin{itemize}
                \item \textbf{Value Based}: No policy, value function
                \item \textbf{Policy Based}: Policy, no value function
                \item \textbf{Actor Critic}: Policy, value function
                \item \textbf{Model Based}: Transition and Reward Model
                \item \textbf{Model Free}: No transition and no reward model
            \end{itemize}
    
    \subsection{Model Free Evaluation}
        Given a policy $\pi$, estimate $V^{\pi}(s)$ without any transition or reward model.

        \subsubsection{Monte Carlo Evaluation}
            We know that in theory we can estimate the value at state $s$ by taking the expectation with respect to $\pi$
            of the discounted sum of the rewards obtained at every time step. Since we don't have the transition and
            reward model we can instead obtain sample approximations where I can replace the expectation by an average. 
            \begin{align*}
            V^{\pi}(s) &= E_{\pi}[\sum_t \gamma^t r_t] \\
            & \approx \frac{1}{n(s)} \sum^{n(s)}_{k=1} \left[\sum_t \gamma^t r_t^{(k)}\right] 
            \end{align*}

            \textbf{Algorithm:}
            
            Let $G_k$ be a one-trajectory Monte Carlo target (execute the policy once).
            $$ G_k = \sum_t \gamma^t r_t^{(k)} $$

            Now we want to have a running average, that I can update gradually one sample at a time we can rewrite the
            approximate value function. This leads to the following incremental update:
            $$ V_{n}^{\pi}(s) \leftarrow V_{n-1}^{\pi}(s)+\alpha_{n}\left(G_{n}-V_{n-1}^{\pi}(s)\right) $$

            where $\alpha_n$ is the learning rate $\frac{1}{n(s)}$.

        \subsubsection{Temporal Difference Evaluation}
            For temporal difference evaluation we will consider the equation for policy evaluation where the value of
            policy $\pi$ is the expected immediate reward plus the discount factor times an expectation with respect to
            future rewards. Now considering the reinforcement learning problem, we no longer have distributions to
            compute this expectation. What instead we can do is take a one sample approximation and replace the
            expectation with one sample approximation for immediate reward and future reward. 

            \begin{align*}
            V^{\pi}(s) &=E[r \mid s, \pi(s)]+\gamma \sum_{s^{\prime}} \operatorname{P}\left(s^{\prime} \mid s, \pi(s)\right) V^{\pi}\left(s^{\prime}\right) \\
            & \approx r+\gamma V^{\pi}\left(s^{\prime}\right)
            \end{align*}

            Using the one sample update we can represent the incremental update as follows:
            $$ V_{n}^{\pi}(s) \leftarrow V_{n-1}^{\pi}(s)+\alpha_{n}\left(r+\gamma
            V_{n-1}^{\pi}\left(s^{\prime}\right)-V_{n-1}^{\pi}(s)\right) $$

            Theorem: If $\alpha_{n}$ is appropriately decreased with number of times a state is visited then
            $V_{n}^{\pi}(s)$ converges to correct value.
            
            Sufficient conditions for $\alpha_{n}$
            \begin{enumerate}
                \item $\sum_{n} \alpha_{n} \rightarrow \infty$
                \item $\sum_{n}\left(\alpha_{n}\right)^{2}<\infty$
            \end{enumerate}

            Often $\alpha_{n}(s)=1 / n(s)$ where $n(s)=\#$ of times $s$ is visited

            \begin{algorithm}[H] \label{TDEval}
                \SetAlgoLined
                \SetKwRepeat{Do}{do}{while}
                \Do {
                    Until convergence of $V^{\pi}$
                } {
                    Execute $\pi(s)$ \;
                    Observe $s^{\prime}$ and $r$\;
                    Update Counts: $n(s) \gets n(s) + 1$ \;
                    Learning Rate: $\alpha \gets 1/n(s)$ \;
                    Update value: $V_{n}^{\pi}(s) \leftarrow V_{n-1}^{\pi}(s)+\alpha_{n}\left(r+\gamma
                    V_{n-1}^{\pi}\left(s^{\prime}\right)-V_{n-1}^{\pi}(s)\right)$ \;
                    $s \gets s^{\prime}$ \;
                }
                \Return $V^{\pi}$
                \caption{TD Evaluation ($\pi, V^{\pi}$)}
            \end{algorithm}

        Comparing Monte Carlo and Temporal Difference evaluation we see the following:

        Monte Carlo Evaluation:
        \begin{itemize}
            \item Unbiased estimate
            \item High Variance
            \item Needs many trajectories
        \end{itemize}

        Temporal Difference Evaluation
        \begin{itemize}
            \item Biased estimate
            \item Lower Variance
            \item Needs less trajectories
        \end{itemize}
    
    \subsection{Model Free Control}
        Now we will consider how to update a policy. We still do not know what the model is but we have some samples that
        will guide us. Instead of evaluating the state value function $V^{\pi}(s)$, evaluate the state-action value
        function $Q^{\pi}(s, a)$.

        We define $Q^{\pi}(s, a)$ as the value of executing $a$ followed by $\pi$. The difference here is that the value
        function was the value of just executing the policy at every step but now with the $Q$ function we are going to
        allow the first action to be different than the one prescribed by policy $\pi$. 
        $$ Q^{\pi}(s, a) = E[r \mid s, a] + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s, a) V^{\pi}(s^{\prime}) $$
        
        In the $Q$ function, the reward is conditioned on the action and we are going to consider all the possible
        actions. We do not necessarily take the maximal action or the action prescribed by a policy; we want to consider
        all possible actions. Now based on the $Q$ function we can compare $Q$ values to find good ways to improve or
        select actions. 

        Greedy policy $\pi^{\prime}$:
        $$ \pi^{\prime}(s) = argmax_a Q^{\pi}(s,a) $$

        \subsubsection{Bellman's Equation}
            For the optimal state-action value function $Q^*(s, a)$ 

            We consider all possible actions $E[r \mid s, a]$ and then follow the optimal policy by taking the
            expectation with respect to what will be the best in the future (maximizing with respect to $a^{\prime})$.

            $$ Q^*(s, a) = E[r \mid s, a] + \gamma \sum_{s^{\prime}} P(s^{\prime} \mid s, a) max_{a^{\prime}}
            Q^*(s^{\prime}, a^{\prime}) $$
        
        \subsubsection{Monte Carlo Control}
            Let $G^a_n$ be a one-trajectory Monte Carlo target.
            $$ G^a_n = r_0^n + \sum_{t=1} \gamma^t r_t^n $$

            where $r_0^n$ is the initial action and $\sum_{t=1} \gamma^t r_t^n$ is the policy $\pi$.

            We can now alternate between:

            Policy evaluation
            $$ Q_{n}^{\pi}(s, a) \leftarrow Q_{n-1}^{\pi}(s, a)+\alpha_{n}\left(G_{n}^{a}-Q_{n-1}^{\pi}(s, a)\right) $$
            Policy improvement
            $$ \pi^{\prime}(s) \leftarrow \operatorname{argmax}_{a} Q^{\pi}(s, a) $$

            The idea here is that we are evaluating our policy by retrieving a new sample at every time step $n$. We
            compare the sample to the current estimate, take the difference and then make a step that will correct based
            on that difference with learning rate $\alpha_n$. So each time a new trajectory is obtained a new $G_n^a$ is
            obtained and this update allows us to gradually evaluate the policy. Now that we have a $Q^{\pi}_n(s,a)$
            estimate, we know the value of different actions and we can simply change the policy to select the best
            action using policy improvement.

        \subsubsection{Temporal Difference Control}
            Temporal Difference control is more efficient and generally used in practice. We can approximate the
            Q-function with a one sample approximation as follows:

            \begin{align*}
            Q^{*}(s, a) &=E[r \mid s, a]+\gamma \sum_{s^{\prime}} \operatorname{P}\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) \\
            & \approx r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)
            \end{align*}
            
            We will get many samples over time so each time we are going to gradually update the Q function using the
            following incremental update:
            $$ Q_{n}^{*}(s, a) \leftarrow Q_{n-1}^{*}(s, a)+\alpha_{n}\left(r+\gamma \max _{a^{\prime}} Q_{n-1}^{*}\left(s^{\prime}, a^{\prime}\right)-Q_{n-1}^{*}(s, a)\right)$$
        
        \subsubsection{Q-Learning Algorithm} \label{QLearning}
            We can leverage the incremental update concept using in temporal difference control to derive the algorithm
            for Q-Learning.
            
            \begin{algorithm}[H] \label{QLearningAlgo}
                \SetAlgoLined
                \SetKwRepeat{Do}{do}{while}
                \Do {
                    Until convergence of $Q^*$
                } {
                    Select and execute $a$ \;
                    Observe $s^{\prime}$ and $r$\;
                    Update Counts: $n(s, a) \gets n(s, a) + 1$ \;
                    Learning Rate: $\alpha \gets 1/n(s,a)$ \;
                    Update Q value: $Q_{n}^{*}(s, a) \leftarrow Q_{n-1}^{*}(s, a)+\alpha_{n}\left(r+\gamma \max
                    _{a^{\prime}} Q_{n-1}^{*}\left(s^{\prime}, a^{\prime}\right)-Q_{n-1}^{*}(s, a)\right)$ \;
                    $s \gets s^{\prime}$ \;
                }
                \Return $Q^*$
                \caption{QLearning($s, Q^*$)}
            \end{algorithm}

            How do we select $a$?
            \begin{itemize}
                \item If an agent always chooses the action with the highest value then it is exploiting. The learned model is not
                the real model and may lead to suboptimal results. (higher rewards in the short term)
                \item By taking random actions (pure exploration) an agent may learn the model. 
            \end{itemize}

            Common Exploration Methods:
            \begin{itemize}
                \item $\epsilon$-greedy: With a probability $\epsilon$ execute random action or otherwise execute best action.
                $$ a^* = argmax_a Q(s,a) $$
                \item Boltzmann Exploration: probability of choosing any action according to the following formula which
                tends to prefer actions with high $Q$ values.
                $$ \operatorname{P}(a)=\frac{e^{\frac{Q(s, a)}{T}}}{\sum_{a} e^{\frac{Q(s, a)}{T}}} $$ 
            \end{itemize}

\section{Deep Q Networks}
    The objective of Deep Q Networks is to estimate the $Q$ function using a neural network. We have seen that we can
    train a neural network using gradient descent. So the objective for a deep $Q$ network is to train the
    \hyperref[QLearning]{Q Learning} algorithm using gradient descent for linear and non-linear objectives.
    
    \subsection{Gradient Q-Learning}
        We have seen the following:
        \begin{itemize}
            \item Q-value estimate: $Q_{\boldsymbol{w}}(s,a)$ 
            \item Target: $r + \gamma \; max_{a^{\prime}} Q_{\boldsymbol{\bar{w}}}(s^{\prime}, a^{\prime})$
        \end{itemize}

        We want to minimize the squared error between the Q-value estimate and target. We use the squared error function
        as our loss function that we want to minimize where we vary $\boldsymbol{w}$ to minimize our error and
        $\bar{\boldsymbol{w}}$ remains fixed.

        Squared Error:
        $$ Err(\boldsymbol{w}) = \frac{1}{2}[Q_{\boldsymbol{w}}(s,a) - r - \gamma \; max_{a^{\prime}}
        Q_{\boldsymbol{\bar{w}}}(s^{\prime}, a^{\prime})]^2 $$

        Gradient
        $$ \frac{\partial Err}{\partial \boldsymbol{w}}=\left[Q_{\boldsymbol{w}}(s, a)-r-\gamma \max _{a^{\prime}}
        Q_{\bar{\boldsymbol{w}}}\left(s^{\prime}, a^{\prime}\right)\right] \frac{\partial Q_{\boldsymbol{w}}(s,
        a)}{\partial \boldsymbol{w}} $$

        The gradient is intuitively the gradient of the $Q$ function times the temporal difference.

        \subsubsection{Gradient Q-Learning Algorithm}
            \begin{algorithm}[H] \label{GradientQLearningAlgo}
                \SetAlgoLined
                \SetKwRepeat{Do}{do}{while}
                Initialize weights $\boldsymbol{w}$ uniformly at random in $[-1,1]$
                Observe current state $s$.
                \Do {
                    Until number of epochs
                } {
                    Select and execute action $a$ \;
                    Receive immediate reward $r$ \;
                    Observer new state $s^{\prime}$ \;
                    Gradient: $\frac{\partial Err}{\partial \boldsymbol{w}}=\left[Q_{\boldsymbol{w}}(s, a)-r-\gamma \max _{a^{\prime}}
                    Q_{\boldsymbol{w}}\left(s^{\prime}, a^{\prime}\right)\right] \frac{\partial Q_{\boldsymbol{w}}(s,
                    a)}{\partial \boldsymbol{w}}$ \;
                    Update weights: $\boldsymbol{w} \gets \boldsymbol{w} - \alpha \frac{\partial Err}{\partial
                    \boldsymbol{w}}$
                    Update state: $ s \gets s^{\prime} $
                }
                \caption{Gradient QLearning($s$)}
            \end{algorithm}
            

            In this case we notice that the target value does not have fixed weights. This can lead to variable
            divergence. If we consider linear gradient $Q$ learning where $Q_{\boldsymbol{w}}(s,a) = \sum_i w_i x_i$
            then the $Q$ learning algorithm converges under the conditions of $\sum_{t=0}^{\infty} \alpha_t = \infty$
            and $\sum_{t=0}^{\infty} \alpha_t^2 < \infty$. However, under these conditions a non-linear gradient $Q$
            learning algorithm may diverge. This is because we haven't fixed $\boldsymbol{w}$ for the target. We are
            effectively changing the target and the estimate.
        
        \subsubsection{Mitigating Divergence}
            To mitigate the divergence issue discussed in the gradient $Q$ learning algorithm we can use 2 tricks. 

            \begin{enumerate}
                \item Experience Replay
                \item Use 2 networks: \begin{itemize}
                    \item Q-Network
                    \item Target Network
                \end{itemize}
            \end{enumerate}

            \textbf{Experience Replay}: The idea is to store previous experiences ($s, a, s^{\prime}, r$) into a buffer
            and sample a mini-batch of previous experiences at each step to learn by $Q$-learning. The intuition behind
            this is that if we introduce some errors elsewhere, a simple way of correcting this is to remember all of
            our previous tuples and fixing the errors by replaying this experience and performing further updates. 
            
            The advantages of this are:
            \begin{itemize}
                \item Break correlations between successive updates (more stable learning)
                \item Fewer interactions with environment needed to converge (greater data efficiency)
            \end{itemize}

            \textbf{Target Network}: The idea is the use a separate target network that is updated only periodically.
            We are going to update our estimate network while keeping the target network fixed and then once in a while
            we are going to update the target network in the direction of the estimation network.

            Repeat for each ($s, a, s^{\prime}, r$) in mini-batch (similar to value iteration):
            \begin{align*}
                \boldsymbol{w} & \leftarrow \boldsymbol{w}-\alpha_{t}\left[Q_{\boldsymbol{w}}(s, a)-r-\gamma \max
                _{a^{\prime}} Q_{\bar{w}}\left(s^{\prime}, a^{\prime}\right)\right] \frac{\partial Q_{\boldsymbol{w}}(s,
                a)}{\partial \boldsymbol{w}} \\
                \bar{\boldsymbol{w}} & \gets \boldsymbol{w}
            \end{align*}

        \subsection{Deep Q-Network Algorithm}
            This algorithm is similar to the \hyperref[GradientQLearningAlgo]{Gradient $Q$ Learning Algo} but no
            incorporates Experience Relay and Target Networks to mitigate the chance of divergence. 

            \begin{algorithm}[H] \label{DeepQLearning}
                \SetAlgoLined
                \SetKwRepeat{Do}{do}{while}
                Initialize weights $\boldsymbol{w}$ uniformly at random in $[-1,1]$
                Observe current state $s$.
                \Do {
                    Until number of epochs
                } {
                    Select and execute action $a$ \;
                    Receive immediate reward $r$ \;
                    Observer new state $s^{\prime}$ \;
                    Add ($s, a, s^{\prime}, r$) to experience buffer \;
                    Sample mini-batch to experiences from buffer \;
                    For each experience ($\hat{s}, \hat{a}, \hat{s}^{\prime}, \hat{r}$) \;
                        \Indp
                        Gradient: $\frac{\partial Err}{\partial \boldsymbol{w}}=\left[Q_{\boldsymbol{w}}(\hat{s}, \hat{a})-\hat{r}-\gamma \max _{\hat{a}^{\prime}}
                        Q_{\bar{\boldsymbol{w}}}\left(\hat{s}^{\prime}, \hat{a}^{\prime}\right)\right] \frac{\partial Q_{\boldsymbol{w}}(\hat{s},
                        \hat{a})}{\partial \boldsymbol{w}}$ \;
                        Update weights: $\boldsymbol{w} \gets \boldsymbol{w} - \alpha \frac{\partial Err}{\partial
                        \boldsymbol{w}}$ \;
                        \Indm
                    Update state: $ s \gets s^{\prime} $
                    Every $c$ steps, update target: $\bar{\boldsymbol{w}} \gets \boldsymbol{w}$
                }
                \caption{Gradient QLearning($s$)}
            \end{algorithm}
    
\section{Policy Gradient Methods}
    Policy gradient is a model-free policy-based method, where no explicit value function representation.

    \subsection{Stochastic Policy}
        When we want to consider the gradient of a policy we want to consider a continuous function (e.g. a stochastic
        policy) since it is a continuous probability between 0 and 1. We can consider the stochastic policy
        $\pi_{\theta}(a \mid s) = P(a \mid s, \theta)$ parametrized by $\theta$.

        If we have finitely many discrete actions then we ca represent the policy as the softmax function:
        $$ \pi_{\theta}(a \mid s) = \frac{exp(h(s,a; \theta))}{\sum_{a^{\prime}} exp(h(s,a^{\prime}; \theta))} $$

        Where $h(s,a;\theta)$ might be linear in $\theta$ or non-linear in $\theta$. 
        
        If we have continuos actions then we can represent it with a Gaussian. We don't have to make the stochastic
        policy continuos if we want to consider continuous functions.

        $$ \pi_{\theta}(a \mid s) = N(a \mid \mu(s;\theta), \sum(s;\theta)) $$

    \subsection{RL Approach}
        Now to generate the Reinforcement Learning Algorithm, we consider a stochastic policy $\pi_{\theta}(a \mid s)$.
        We have data in state-action-reward triples as $\{(s_1, a_1, r_1), (s_2, a_2, r_2), ... \}$.

        We want to maximize the discounted sum of rewards
        $$ \theta^* = argmax_{\theta} \sum_n \gamma^n E_{\theta}[r_n \mid s_n, a_n] $$

        Taking the derivative of this we get the gradient update:
        $$ \theta_{n+1} \gets \theta_n + \alpha_n \gamma^n G_n \nabla_{\theta} log \pi_{\theta}(a_n \mid s_n) $$
        where $G_n = \sum_{t=0}^{\infty} \gamma^t r_{n+t}$

        This can be derived using the stochastic gradient policy theorem. Which shows:
        $$ \nabla V_{\theta} \approx \gamma^n G_n \nabla log \pi_{\theta} (a_n | s_n) $$

        \subsubsection{Algorithm}
            \begin{algorithm}[H] \label{REINFORCE}
            \SetAlgoLined
            \SetKwRepeat{Do}{do}{while}
            Initialize $\pi_{\theta}$ to anything \;
            \Do {
                forever (for each episode)
            } {
                Generate episode $s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T$ with $\pi_{\theta}$ \;
                \Do{
                    For each step of the episode $n = 0, 1, ... T$
                } {
                    $G_n \gets \sum_{t=0}^{T-n} \gamma^t r_{n+t}$ \;
                    Update policy: $\theta \gets \theta + \alpha \gamma^n G_n \nabla log \pi_{\theta}(a_n \mid s_n)$ \;
                }
            }
            Return $\pi(\theta)$ \;
            \caption{REINFORCE($s_0, \pi_{\theta}$)}
        \end{algorithm}

    \subsection{Policy Network}
        A policy network is essentially a neural network that encodes a policy that would indicate with what probability
        an action is selected given an environment configuration. The input being the state $s$ and the output being a
        distribution over the actions $a$. 

        \subsubsection{Training}
            Let $G_n = \sum_t \gamma^t r_{n+t}$ be the discounted sum of rewards in trajectory that starts in $s$ at
            time $n$ by executing $a$. 

            Gradient: 
            $$\nabla \theta = \frac{\partial{log \pi_{\theta}(a \mid s)}}{\partial \theta} \gamma^n G_n$$

            The intuition is that we are rescaling the supervised learning gradient by $G_n$.

            Policy Update:
            $$ \theta \gets \theta + \alpha \nabla \theta $$

    \subsection{Value Network}
        The value network would predict $V(s^{\prime})$ which would indicate who will win each game in each state
        $s^{\prime}$. It would take in an input state $s$ and output the expected sum of rewards $V(s^{\prime})$.
        
        \subsubsection{Training}
            Let $\boldsymbol{w}$ be the weights of the value network

            Then we have the data $(s, G)$ where $G$ can be something like [-1, 1] for loss and win respectively.

            Objective: minimize $\frac{1}{2} (V_{\boldsymbol{w}}(s) - G)^2$ \\
            Gradient: $\nabla \boldsymbol{w} = \frac{\partial V_{\boldsymbol{w}}(s)}{\partial \boldsymbol{w}}
            (V_{\boldsymbol{w}}(s) - G)$ \\
            Weight Update: $\boldsymbol{w} \gets \boldsymbol{w} - \alpha \nabla \boldsymbol{w}$

    We can combine the policy and value networks into a monte carlo tree search algorithm. 

\printindex

\end{document}