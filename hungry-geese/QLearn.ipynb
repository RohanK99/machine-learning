{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n",
    "from kaggle_environments import evaluate, make, utils\n",
    "\n",
    "from enum import auto, Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    GOOSE = auto()\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, alpha, gamma, epsilon):\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_size = 4 # (EAST, WEST, NORTH, SOUTH)\n",
    "        self.prev_action = None\n",
    "        self.FOV = 2\n",
    "        self.rows = 7\n",
    "        self.columns = 11\n",
    "\n",
    "    def encode_state(self, observation):\n",
    "        \"\"\"\n",
    "        Encode state of size FOV (includes wrapping)\n",
    "        \"\"\"\n",
    "        state = []\n",
    "        player_goose = observation.geese[observation.index]\n",
    "        player_head = player_goose[0]\n",
    "        row_0, col_0 = row_col(player_head, self.columns)\n",
    "        for row_delta in range (-self.FOV, self.FOV+1):\n",
    "            for col_delta in range (-self.FOV, self.FOV+1):\n",
    "                row_i = (row_0+row_delta)%self.rows\n",
    "                col_i = (col_0+col_delta)%self.columns\n",
    "\n",
    "                pos = self.columns*row_i+col_i\n",
    "                goose_cells = [cell for geese in observation.geese for cell in geese]\n",
    "                if pos in goose_cells:\n",
    "                    state.append(Cell.GOOSE)\n",
    "                elif pos in observation.food:\n",
    "                    state.append(Cell.FOOD)\n",
    "                else:\n",
    "                    state.append(Cell.EMPTY)\n",
    "\n",
    "        state = \"\".join([str(s.value) for s in state])\n",
    "        return state\n",
    "\n",
    "    def get_action(self, obs, cfg=None):\n",
    "        \"\"\"\n",
    "        Returns action for training using exploration and exploitation\n",
    "        \"\"\"\n",
    "        state = self.encode_state(obs)\n",
    "\n",
    "        if not state in self.q_table:\n",
    "            print(\"state doesn't exist, choosing random action\")\n",
    "            self.q_table[state] = [0]*self.action_size\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            if (random.random() < self.epsilon):\n",
    "                print(\"exploration\")\n",
    "                action = random.choice(range(self.action_size))\n",
    "            else:\n",
    "                print(\"exploitation\")\n",
    "                q_state = self.q_table[state]\n",
    "                action = np.argmax(q_state)\n",
    "        \n",
    "        # make sure action is not the opposite of the previous action\n",
    "        if (self.prev_action == None):\n",
    "            self.prev_action = Action(action+1)\n",
    "        else:\n",
    "            while (Action(action+1) == self.prev_action.opposite()):\n",
    "                print(\"opposite action trying again :(\")\n",
    "                action = random.choice(range(self.action_size))\n",
    "            \n",
    "            self.prev_action = Action(action+1)\n",
    "        \n",
    "        return Action(action+1).name\n",
    "\n",
    "    def get_action_max(self, obs, cfg=None):\n",
    "        \"\"\"\n",
    "        Returns action using exploitation for testing\n",
    "        \"\"\"\n",
    "        state = self.encode_state(obs)\n",
    "\n",
    "        if not state in self.q_table:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            q_state = self.q_table[state]\n",
    "            action = np.argmax(q_state)\n",
    "        \n",
    "        return Action(action+1).name\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, next_state, reward = experience\n",
    "        state = self.encode_state(state)\n",
    "        next_state = self.encode_state(next_state)\n",
    "\n",
    "        q_next = self.q_table.get(next_state, 0)\n",
    "        q_target = reward + self.gamma * np.max(q_next)\n",
    "        q_temporal_diff = q_target - self.q_table[state][Action[action].value-1]\n",
    "\n",
    "        self.q_table[state][Action[action].value-1] += self.alpha * q_temporal_diff\n",
    "\n",
    "    def translate_goose(self, pos, action):\n",
    "        row, col = row_col(pos, self.columns)\n",
    "        if (action == 'NORTH'):\n",
    "            row = row - 1 % self.rows\n",
    "        elif (action == 'SOUTH'):\n",
    "            row = row + 1 % self.rows\n",
    "        elif (action == 'WEST'):\n",
    "            col = col - 1 % self.columns\n",
    "        else:\n",
    "            col = col + 1 % self.columns\n",
    "\n",
    "        translated_pos = self.columns*row+col\n",
    "        return translated_pos\n",
    "\n",
    "    def compute_reward(self, state, action):\n",
    "        player_goose = state.geese[state.index]\n",
    "        player_head = player_goose[0]\n",
    "        pos = self.translate_goose(player_head, action)\n",
    "        reward = 0\n",
    "\n",
    "        goose_cells = [cell for geese in state.geese for cell in geese]\n",
    "        if pos in goose_cells:\n",
    "            # negative reward for colliding\n",
    "            reward = -10\n",
    "        elif pos in state.food:\n",
    "            # reward for eating food\n",
    "            reward = 10\n",
    "        else:\n",
    "            # reward +1 for moving towards food\n",
    "            orig_row, orig_col = row_col(player_head, self.columns)\n",
    "            trans_row, trans_col = row_col(pos, self.columns)\n",
    "            for row_delta in range (-self.FOV, self.FOV+1):\n",
    "                for col_delta in range (-self.FOV, self.FOV+1):\n",
    "                    row_i = (orig_row+row_delta)%self.rows\n",
    "                    col_i = (orig_col+col_delta)%self.columns\n",
    "\n",
    "                    cell_pos = self.columns*row_i+col_i\n",
    "                    if (cell_pos in state.food):\n",
    "                        food_row, food_col = row_col(cell_pos, self.columns)\n",
    "\n",
    "                        orig_dist = abs(orig_row-food_row) + abs(orig_col-food_col)\n",
    "                        trans_dist = abs(trans_row-food_row) + abs(trans_col-food_col)\n",
    "                        \n",
    "                        if (trans_dist < orig_dist):\n",
    "                            reward = 1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def save_pickle(self, name):\n",
    "        save_data = self.q_table\n",
    "        with open(f'{name}', 'wb') as handle:\n",
    "            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load_pickle(self, name):\n",
    "        with open(f'{name}', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.q_table = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(alpha=0.05, gamma=0.8, epsilon=0.1)\n",
    "agent.load_pickle(\"qtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the qtable\n",
    "env = make(\"hungry_geese\", debug=True)\n",
    "\n",
    "trainer = env.train([None, \"greedy\"]) # modify num goose in training\n",
    "EPISODES = 500000\n",
    "episodes = tqdm(range(EPISODES))\n",
    "for eps in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    state = trainer.reset()\n",
    "    while not env.done:\n",
    "        action = agent.get_action(state)\n",
    "        train_reward = agent.compute_reward(state, action)\n",
    "        next_state, reward, done, _ = trainer.step(action)\n",
    "        episode_reward += reward\n",
    "        if (env.done):\n",
    "            break\n",
    "        agent.train((state, action, next_state, train_reward))\n",
    "        state = next_state\n",
    "    \n",
    "    print(episode_reward)\n",
    "\n",
    "agent.save_pickle(\"qtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test goose based on trained qtable\n",
    "test_env = make(\"hungry_geese\", debug=False)\n",
    "test_env.run([agent.get_action_max, \"greedy\"])\n",
    "test_env.render(mode=\"ipython\", width=500, height=400)"
   ]
  }
 ]
}